{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.python.keras import layers, models\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123, 33, 1)\n"
     ]
    }
   ],
   "source": [
    "save_path = \"C:/Users/13764/Documents/academics/zhejiang/GP-WP/GP-WP data/dataset_0.npy\"\n",
    "temp = np.load(save_path, allow_pickle = True)\n",
    "res_dict_dataset = temp.item()\n",
    "data_output = res_dict_dataset[\"data_output\"]\n",
    "train_output = res_dict_dataset[\"train_output\"]\n",
    "test_output = res_dict_dataset[\"test_output\"]\n",
    "data_input = res_dict_dataset[\"data_input\"]\n",
    "train_input = res_dict_dataset[\"train_input\"]\n",
    "test_input = res_dict_dataset[\"test_input\"]\n",
    "\n",
    "# adds a dummy dimension for cnn\n",
    "train_input = train_input[..., None]\n",
    "test_input = test_input[...,None]\n",
    "data_input = data_input[..., None]\n",
    "print(train_input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "4/4 [==============================] - 1s 54ms/step - loss: 5809.5576 - rmse: 76.6463 - val_loss: 30654.7715 - val_rmse: 103.3497\n",
      "Epoch 2/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 5617.1006 - rmse: 95.0992 - val_loss: 29427.4297 - val_rmse: 102.3889\n",
      "Epoch 3/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 5328.5449 - rmse: 97.3637 - val_loss: 27765.5137 - val_rmse: 101.1516\n",
      "Epoch 4/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 4913.5986 - rmse: 97.4137 - val_loss: 25621.4551 - val_rmse: 99.5852\n",
      "Epoch 5/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 4344.8125 - rmse: 96.2963 - val_loss: 22969.0625 - val_rmse: 97.6376\n",
      "Epoch 6/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 3667.1826 - rmse: 94.9088 - val_loss: 19776.3711 - val_rmse: 95.2990\n",
      "Epoch 7/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2877.9946 - rmse: 92.8442 - val_loss: 16138.1963 - val_rmse: 92.5584\n",
      "Epoch 8/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1989.2427 - rmse: 90.1509 - val_loss: 12293.4102 - val_rmse: 89.4281\n",
      "Epoch 9/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1209.9711 - rmse: 87.1926 - val_loss: 8523.8730 - val_rmse: 86.0382\n",
      "Epoch 10/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 651.7358 - rmse: 83.9284 - val_loss: 5381.7261 - val_rmse: 82.5847\n",
      "Epoch 11/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 453.1757 - rmse: 80.6856 - val_loss: 3384.3501 - val_rmse: 79.3326\n",
      "Epoch 12/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 533.0350 - rmse: 77.6952 - val_loss: 2605.6802 - val_rmse: 76.4688\n",
      "Epoch 13/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 584.3924 - rmse: 75.0435 - val_loss: 2636.5232 - val_rmse: 73.9836\n",
      "Epoch 14/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 496.7545 - rmse: 72.7007 - val_loss: 3128.8257 - val_rmse: 71.7980\n",
      "Epoch 15/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 400.6970 - rmse: 70.6262 - val_loss: 3767.9536 - val_rmse: 69.8714\n",
      "Epoch 16/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 367.6203 - rmse: 68.7954 - val_loss: 4231.8730 - val_rmse: 68.1705\n",
      "Epoch 17/300\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 367.7104 - rmse: 67.1858 - val_loss: 4312.5625 - val_rmse: 66.6407\n",
      "Epoch 18/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 362.8613 - rmse: 65.7331 - val_loss: 4104.4976 - val_rmse: 65.2317\n",
      "Epoch 19/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 342.3573 - rmse: 64.3997 - val_loss: 3682.1099 - val_rmse: 63.9039\n",
      "Epoch 20/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 321.0996 - rmse: 63.1221 - val_loss: 3203.8723 - val_rmse: 62.6406\n",
      "Epoch 21/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 304.5877 - rmse: 61.9145 - val_loss: 2858.8916 - val_rmse: 61.4438\n",
      "Epoch 22/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 292.5257 - rmse: 60.7600 - val_loss: 2689.3250 - val_rmse: 60.3191\n",
      "Epoch 23/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 282.4298 - rmse: 59.6837 - val_loss: 2582.3120 - val_rmse: 59.2629\n",
      "Epoch 24/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 269.3469 - rmse: 58.6649 - val_loss: 2562.5891 - val_rmse: 58.2727\n",
      "Epoch 25/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 258.8573 - rmse: 57.7081 - val_loss: 2554.7327 - val_rmse: 57.3432\n",
      "Epoch 26/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 248.0385 - rmse: 56.8109 - val_loss: 2480.2883 - val_rmse: 56.4637\n",
      "Epoch 27/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 237.9865 - rmse: 55.9513 - val_loss: 2387.4717 - val_rmse: 55.6282\n",
      "Epoch 28/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 229.0345 - rmse: 55.1417 - val_loss: 2260.9165 - val_rmse: 54.8305\n",
      "Epoch 29/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 220.8843 - rmse: 54.3753 - val_loss: 2145.6270 - val_rmse: 54.0680\n",
      "Epoch 30/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 212.6563 - rmse: 53.6334 - val_loss: 2037.1133 - val_rmse: 53.3377\n",
      "Epoch 31/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 203.9701 - rmse: 52.9177 - val_loss: 1975.6501 - val_rmse: 52.6396\n",
      "Epoch 32/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 196.3177 - rmse: 52.2405 - val_loss: 1899.8708 - val_rmse: 51.9703\n",
      "Epoch 33/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 188.4515 - rmse: 51.5848 - val_loss: 1846.2460 - val_rmse: 51.3286\n",
      "Epoch 34/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 182.3552 - rmse: 50.9621 - val_loss: 1773.6354 - val_rmse: 50.7117\n",
      "Epoch 35/300\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 176.0680 - rmse: 50.3616 - val_loss: 1714.0618 - val_rmse: 50.1183\n",
      "Epoch 36/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 169.4817 - rmse: 49.7766 - val_loss: 1693.9952 - val_rmse: 49.5488\n",
      "Epoch 37/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 164.3338 - rmse: 49.2233 - val_loss: 1651.2162 - val_rmse: 49.0005\n",
      "Epoch 38/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 157.7958 - rmse: 48.6833 - val_loss: 1563.6073 - val_rmse: 48.4693\n",
      "Epoch 39/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 153.4534 - rmse: 48.1669 - val_loss: 1485.7048 - val_rmse: 47.9549\n",
      "Epoch 40/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 148.1291 - rmse: 47.6616 - val_loss: 1454.4977 - val_rmse: 47.4582\n",
      "Epoch 41/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 143.7213 - rmse: 47.1745 - val_loss: 1447.0416 - val_rmse: 46.9797\n",
      "Epoch 42/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 139.3741 - rmse: 46.7075 - val_loss: 1396.4370 - val_rmse: 46.5159\n",
      "Epoch 43/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 135.2725 - rmse: 46.2505 - val_loss: 1372.5648 - val_rmse: 46.0673\n",
      "Epoch 44/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 131.0027 - rmse: 45.8117 - val_loss: 1332.2808 - val_rmse: 45.6321\n",
      "Epoch 45/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 127.5687 - rmse: 45.3840 - val_loss: 1294.7355 - val_rmse: 45.2100\n",
      "Epoch 46/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 123.8641 - rmse: 44.9672 - val_loss: 1279.3344 - val_rmse: 44.8009\n",
      "Epoch 47/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 120.8649 - rmse: 44.5692 - val_loss: 1242.0095 - val_rmse: 44.4035\n",
      "Epoch 48/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 117.5998 - rmse: 44.1764 - val_loss: 1231.1311 - val_rmse: 44.0181\n",
      "Epoch 49/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 114.2846 - rmse: 43.7973 - val_loss: 1192.9000 - val_rmse: 43.6428\n",
      "Epoch 50/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 111.3884 - rmse: 43.4275 - val_loss: 1172.2124 - val_rmse: 43.2781\n",
      "Epoch 51/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 109.0773 - rmse: 43.0723 - val_loss: 1126.4025 - val_rmse: 42.9222\n",
      "Epoch 52/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 105.8643 - rmse: 42.7194 - val_loss: 1095.6239 - val_rmse: 42.5753\n",
      "Epoch 53/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 103.4764 - rmse: 42.3786 - val_loss: 1082.7483 - val_rmse: 42.2378\n",
      "Epoch 54/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 100.7268 - rmse: 42.0457 - val_loss: 1076.6495 - val_rmse: 41.9094\n",
      "Epoch 55/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 98.4233 - rmse: 41.7213 - val_loss: 1062.0638 - val_rmse: 41.5896\n",
      "Epoch 56/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 95.9429 - rmse: 41.4064 - val_loss: 1018.3690 - val_rmse: 41.2765\n",
      "Epoch 57/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 93.5546 - rmse: 41.0982 - val_loss: 994.3305 - val_rmse: 40.9707\n",
      "Epoch 58/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 91.3835 - rmse: 40.7975 - val_loss: 962.9888 - val_rmse: 40.6716\n",
      "Epoch 59/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 89.4600 - rmse: 40.5019 - val_loss: 955.6678 - val_rmse: 40.3799\n",
      "Epoch 60/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 87.0941 - rmse: 40.2154 - val_loss: 929.3624 - val_rmse: 40.0944\n",
      "Epoch 61/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 85.7483 - rmse: 39.9331 - val_loss: 888.9199 - val_rmse: 39.8145\n",
      "Epoch 62/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 83.1353 - rmse: 39.6559 - val_loss: 908.6695 - val_rmse: 39.5421\n",
      "Epoch 63/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 81.2291 - rmse: 39.3886 - val_loss: 882.0650 - val_rmse: 39.2752\n",
      "Epoch 64/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 79.4987 - rmse: 39.1249 - val_loss: 866.4667 - val_rmse: 39.0140\n",
      "Epoch 65/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 77.2879 - rmse: 38.8664 - val_loss: 821.2833 - val_rmse: 38.7571\n",
      "Epoch 66/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 75.4963 - rmse: 38.6114 - val_loss: 807.6317 - val_rmse: 38.5055\n",
      "Epoch 67/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 73.6033 - rmse: 38.3636 - val_loss: 780.7255 - val_rmse: 38.2585\n",
      "Epoch 68/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 72.2211 - rmse: 38.1202 - val_loss: 744.1321 - val_rmse: 38.0156\n",
      "Epoch 69/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 70.5116 - rmse: 37.8794 - val_loss: 750.6476 - val_rmse: 37.7782\n",
      "Epoch 70/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 68.6145 - rmse: 37.6447 - val_loss: 737.3437 - val_rmse: 37.5454\n",
      "Epoch 71/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 67.1817 - rmse: 37.4146 - val_loss: 713.7403 - val_rmse: 37.3167\n",
      "Epoch 72/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 65.6295 - rmse: 37.1890 - val_loss: 713.1426 - val_rmse: 37.0927\n",
      "Epoch 73/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 64.0102 - rmse: 36.9666 - val_loss: 691.3397 - val_rmse: 36.8726\n",
      "Epoch 74/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 62.4111 - rmse: 36.7489 - val_loss: 655.8683 - val_rmse: 36.6555\n",
      "Epoch 75/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 61.2080 - rmse: 36.5345 - val_loss: 629.6306 - val_rmse: 36.4419\n",
      "Epoch 76/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 59.7786 - rmse: 36.3235 - val_loss: 621.4156 - val_rmse: 36.2323\n",
      "Epoch 77/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 58.1228 - rmse: 36.1159 - val_loss: 621.9393 - val_rmse: 36.0266\n",
      "Epoch 78/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 56.9763 - rmse: 35.9126 - val_loss: 611.3204 - val_rmse: 35.8246\n",
      "Epoch 79/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 55.5395 - rmse: 35.7122 - val_loss: 593.3377 - val_rmse: 35.6258\n",
      "Epoch 80/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 54.3465 - rmse: 35.5155 - val_loss: 570.6408 - val_rmse: 35.4299\n",
      "Epoch 81/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 53.1537 - rmse: 35.3217 - val_loss: 531.9167 - val_rmse: 35.2362\n",
      "Epoch 82/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 51.9285 - rmse: 35.1298 - val_loss: 519.1741 - val_rmse: 35.0457\n",
      "Epoch 83/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 50.6614 - rmse: 34.9409 - val_loss: 507.2763 - val_rmse: 34.8582\n",
      "Epoch 84/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 49.5572 - rmse: 34.7553 - val_loss: 499.0741 - val_rmse: 34.6737\n",
      "Epoch 85/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 48.5243 - rmse: 34.5728 - val_loss: 496.6206 - val_rmse: 34.4924\n",
      "Epoch 86/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 47.4961 - rmse: 34.3931 - val_loss: 476.7065 - val_rmse: 34.3136\n",
      "Epoch 87/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 46.2590 - rmse: 34.2156 - val_loss: 468.5183 - val_rmse: 34.1376\n",
      "Epoch 88/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 45.2542 - rmse: 34.0408 - val_loss: 455.3766 - val_rmse: 33.9641\n",
      "Epoch 89/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 44.5046 - rmse: 33.8695 - val_loss: 442.7527 - val_rmse: 33.7931\n",
      "Epoch 90/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 43.3081 - rmse: 33.6997 - val_loss: 425.5353 - val_rmse: 33.6244\n",
      "Epoch 91/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 42.6007 - rmse: 33.5325 - val_loss: 409.2414 - val_rmse: 33.4579\n",
      "Epoch 92/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 41.7152 - rmse: 33.3672 - val_loss: 392.8661 - val_rmse: 33.2937\n",
      "Epoch 93/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 40.5809 - rmse: 33.2041 - val_loss: 403.0916 - val_rmse: 33.1323\n",
      "Epoch 94/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 39.8730 - rmse: 33.0442 - val_loss: 396.4178 - val_rmse: 32.9733\n",
      "Epoch 95/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 39.2116 - rmse: 32.8868 - val_loss: 383.4002 - val_rmse: 32.8164\n",
      "Epoch 96/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 37.9313 - rmse: 32.7312 - val_loss: 346.4696 - val_rmse: 32.6608\n",
      "Epoch 97/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 37.8238 - rmse: 32.5770 - val_loss: 315.8442 - val_rmse: 32.5066\n",
      "Epoch 98/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 36.8103 - rmse: 32.4239 - val_loss: 317.3016 - val_rmse: 32.3548\n",
      "Epoch 99/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 35.7716 - rmse: 32.2733 - val_loss: 337.7171 - val_rmse: 32.2059\n",
      "Epoch 100/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 35.1973 - rmse: 32.1255 - val_loss: 332.7990 - val_rmse: 32.0591\n",
      "Epoch 101/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 34.4482 - rmse: 31.9796 - val_loss: 309.2642 - val_rmse: 31.9137\n",
      "Epoch 102/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 33.5849 - rmse: 31.8353 - val_loss: 288.7368 - val_rmse: 31.7697\n",
      "Epoch 103/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 32.9908 - rmse: 31.6927 - val_loss: 281.8854 - val_rmse: 31.6277\n",
      "Epoch 104/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 32.1483 - rmse: 31.5517 - val_loss: 279.1747 - val_rmse: 31.4876\n",
      "Epoch 105/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 31.6903 - rmse: 31.4126 - val_loss: 280.0050 - val_rmse: 31.3495\n",
      "Epoch 106/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 31.1565 - rmse: 31.2756 - val_loss: 270.5623 - val_rmse: 31.2131\n",
      "Epoch 107/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 30.0539 - rmse: 31.1400 - val_loss: 237.4079 - val_rmse: 31.0775\n",
      "Epoch 108/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 29.6750 - rmse: 31.0055 - val_loss: 215.7358 - val_rmse: 30.9432\n",
      "Epoch 109/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 29.4577 - rmse: 30.8722 - val_loss: 228.5314 - val_rmse: 30.8111\n",
      "Epoch 110/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 28.3519 - rmse: 30.7406 - val_loss: 223.4151 - val_rmse: 30.6806\n",
      "Epoch 111/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 27.8590 - rmse: 30.6111 - val_loss: 220.1579 - val_rmse: 30.5518\n",
      "Epoch 112/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 27.5845 - rmse: 30.4830 - val_loss: 198.0923 - val_rmse: 30.4240\n",
      "Epoch 113/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 27.0143 - rmse: 30.3567 - val_loss: 211.7100 - val_rmse: 30.2984\n",
      "Epoch 114/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 26.1629 - rmse: 30.2316 - val_loss: 199.8040 - val_rmse: 30.1739\n",
      "Epoch 115/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 25.6311 - rmse: 30.1081 - val_loss: 183.5907 - val_rmse: 30.0506\n",
      "Epoch 116/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 25.1771 - rmse: 29.9857 - val_loss: 159.8986 - val_rmse: 29.9283\n",
      "Epoch 117/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 24.6752 - rmse: 29.8640 - val_loss: 159.9301 - val_rmse: 29.8074\n",
      "Epoch 118/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 24.1308 - rmse: 29.7441 - val_loss: 169.6916 - val_rmse: 29.6884\n",
      "Epoch 119/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 23.8545 - rmse: 29.6257 - val_loss: 165.7546 - val_rmse: 29.5707\n",
      "Epoch 120/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 23.2245 - rmse: 29.5086 - val_loss: 143.3142 - val_rmse: 29.4539\n",
      "Epoch 121/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 22.9129 - rmse: 29.3926 - val_loss: 134.3348 - val_rmse: 29.3382\n",
      "Epoch 122/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 22.2660 - rmse: 29.2777 - val_loss: 149.4583 - val_rmse: 29.2243\n",
      "Epoch 123/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 21.8428 - rmse: 29.1646 - val_loss: 143.7558 - val_rmse: 29.1117\n",
      "Epoch 124/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 21.7462 - rmse: 29.0527 - val_loss: 123.1015 - val_rmse: 28.9999\n",
      "Epoch 125/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 21.1707 - rmse: 28.9412 - val_loss: 118.0406 - val_rmse: 28.8892\n",
      "Epoch 126/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 20.4564 - rmse: 28.8313 - val_loss: 130.3701 - val_rmse: 28.7801\n",
      "Epoch 127/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 20.2776 - rmse: 28.7231 - val_loss: 125.7047 - val_rmse: 28.6722\n",
      "Epoch 128/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 19.9115 - rmse: 28.6158 - val_loss: 110.6307 - val_rmse: 28.5651\n",
      "Epoch 129/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 19.4830 - rmse: 28.5094 - val_loss: 97.2744 - val_rmse: 28.4589\n",
      "Epoch 130/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 19.1040 - rmse: 28.4038 - val_loss: 101.2104 - val_rmse: 28.3540\n",
      "Epoch 131/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 18.6572 - rmse: 28.2995 - val_loss: 101.5005 - val_rmse: 28.2503\n",
      "Epoch 132/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 18.3514 - rmse: 28.1963 - val_loss: 93.0547 - val_rmse: 28.1476\n",
      "Epoch 133/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 17.9717 - rmse: 28.0942 - val_loss: 90.2340 - val_rmse: 28.0459\n",
      "Epoch 134/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 17.7280 - rmse: 27.9932 - val_loss: 92.0047 - val_rmse: 27.9453\n",
      "Epoch 135/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 17.3436 - rmse: 27.8931 - val_loss: 83.9951 - val_rmse: 27.8457\n",
      "Epoch 136/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 17.0864 - rmse: 27.7939 - val_loss: 74.3324 - val_rmse: 27.7469\n",
      "Epoch 137/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 16.7317 - rmse: 27.6957 - val_loss: 73.2252 - val_rmse: 27.6491\n",
      "Epoch 138/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 16.3655 - rmse: 27.5986 - val_loss: 75.8981 - val_rmse: 27.5524\n",
      "Epoch 139/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 16.1413 - rmse: 27.5023 - val_loss: 71.1328 - val_rmse: 27.4566\n",
      "Epoch 140/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 15.9303 - rmse: 27.4070 - val_loss: 65.7518 - val_rmse: 27.3618\n",
      "Epoch 141/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 15.5397 - rmse: 27.3127 - val_loss: 60.0934 - val_rmse: 27.2677\n",
      "Epoch 142/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 15.2974 - rmse: 27.2192 - val_loss: 56.0457 - val_rmse: 27.1746\n",
      "Epoch 143/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 15.0550 - rmse: 27.1264 - val_loss: 55.6241 - val_rmse: 27.0823\n",
      "Epoch 144/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 14.8206 - rmse: 27.0349 - val_loss: 61.4285 - val_rmse: 26.9912\n",
      "Epoch 145/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 14.6727 - rmse: 26.9440 - val_loss: 60.0675 - val_rmse: 26.9010\n",
      "Epoch 146/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 14.5244 - rmse: 26.8544 - val_loss: 48.1848 - val_rmse: 26.8114\n",
      "Epoch 147/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 14.0546 - rmse: 26.7653 - val_loss: 46.2464 - val_rmse: 26.7227\n",
      "Epoch 148/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 13.8805 - rmse: 26.6770 - val_loss: 46.5915 - val_rmse: 26.6348\n",
      "Epoch 149/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 13.7057 - rmse: 26.5895 - val_loss: 41.3695 - val_rmse: 26.5477\n",
      "Epoch 150/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 13.4209 - rmse: 26.5028 - val_loss: 43.2086 - val_rmse: 26.4615\n",
      "Epoch 151/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 13.2462 - rmse: 26.4171 - val_loss: 40.9629 - val_rmse: 26.3761\n",
      "Epoch 152/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 13.0891 - rmse: 26.3321 - val_loss: 36.4952 - val_rmse: 26.2914\n",
      "Epoch 153/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 12.9020 - rmse: 26.2478 - val_loss: 39.9666 - val_rmse: 26.2076\n",
      "Epoch 154/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 12.7219 - rmse: 26.1645 - val_loss: 36.5835 - val_rmse: 26.1245\n",
      "Epoch 155/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 12.6969 - rmse: 26.0818 - val_loss: 29.7819 - val_rmse: 26.0421\n",
      "Epoch 156/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 12.3049 - rmse: 25.9998 - val_loss: 33.0572 - val_rmse: 25.9605\n",
      "Epoch 157/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 12.0631 - rmse: 25.9185 - val_loss: 32.7652 - val_rmse: 25.8797\n",
      "Epoch 158/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 11.9710 - rmse: 25.8381 - val_loss: 31.0275 - val_rmse: 25.7996\n",
      "Epoch 159/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 11.6586 - rmse: 25.7585 - val_loss: 30.4514 - val_rmse: 25.7202\n",
      "Epoch 160/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 11.5142 - rmse: 25.6794 - val_loss: 28.5898 - val_rmse: 25.6415\n",
      "Epoch 161/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 11.4412 - rmse: 25.6011 - val_loss: 25.1157 - val_rmse: 25.5635\n",
      "Epoch 162/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 11.4237 - rmse: 25.5236 - val_loss: 28.2019 - val_rmse: 25.4862\n",
      "Epoch 163/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 11.0877 - rmse: 25.4464 - val_loss: 23.4379 - val_rmse: 25.4096\n",
      "Epoch 164/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 10.8597 - rmse: 25.3703 - val_loss: 23.7008 - val_rmse: 25.3336\n",
      "Epoch 165/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 10.6374 - rmse: 25.2947 - val_loss: 24.5287 - val_rmse: 25.2583\n",
      "Epoch 166/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 10.6509 - rmse: 25.2197 - val_loss: 25.7381 - val_rmse: 25.1837\n",
      "Epoch 167/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 10.4580 - rmse: 25.1453 - val_loss: 20.1592 - val_rmse: 25.1097\n",
      "Epoch 168/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 10.3835 - rmse: 25.0717 - val_loss: 18.8693 - val_rmse: 25.0363\n",
      "Epoch 169/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 10.1150 - rmse: 24.9988 - val_loss: 21.9667 - val_rmse: 24.9636\n",
      "Epoch 170/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 10.0925 - rmse: 24.9264 - val_loss: 21.3641 - val_rmse: 24.8915\n",
      "Epoch 171/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 9.9241 - rmse: 24.8545 - val_loss: 16.9091 - val_rmse: 24.8200\n",
      "Epoch 172/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 9.7282 - rmse: 24.7831 - val_loss: 17.0222 - val_rmse: 24.7490\n",
      "Epoch 173/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 9.5241 - rmse: 24.7126 - val_loss: 19.8034 - val_rmse: 24.6787\n",
      "Epoch 174/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 9.5445 - rmse: 24.6427 - val_loss: 18.2708 - val_rmse: 24.6090\n",
      "Epoch 175/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 9.2260 - rmse: 24.5733 - val_loss: 15.6818 - val_rmse: 24.5398\n",
      "Epoch 176/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 9.3874 - rmse: 24.5044 - val_loss: 14.6097 - val_rmse: 24.4712\n",
      "Epoch 177/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 9.1598 - rmse: 24.4361 - val_loss: 16.7625 - val_rmse: 24.4032\n",
      "Epoch 178/300\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 8.9815 - rmse: 24.3683 - val_loss: 17.2236 - val_rmse: 24.3358\n",
      "Epoch 179/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 8.8073 - rmse: 24.3013 - val_loss: 15.6608 - val_rmse: 24.2689\n",
      "Epoch 180/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 8.6903 - rmse: 24.2346 - val_loss: 14.5708 - val_rmse: 24.2025\n",
      "Epoch 181/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 8.7410 - rmse: 24.1686 - val_loss: 15.1347 - val_rmse: 24.1367\n",
      "Epoch 182/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 8.4968 - rmse: 24.1031 - val_loss: 14.0742 - val_rmse: 24.0714\n",
      "Epoch 183/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 8.3835 - rmse: 24.0379 - val_loss: 13.7929 - val_rmse: 24.0066\n",
      "Epoch 184/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 8.3224 - rmse: 23.9735 - val_loss: 14.9381 - val_rmse: 23.9424\n",
      "Epoch 185/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 8.2409 - rmse: 23.9094 - val_loss: 13.6572 - val_rmse: 23.8787\n",
      "Epoch 186/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 8.0266 - rmse: 23.8461 - val_loss: 13.1413 - val_rmse: 23.8154\n",
      "Epoch 187/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 8.0708 - rmse: 23.7830 - val_loss: 13.0023 - val_rmse: 23.7527\n",
      "Epoch 188/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 7.9536 - rmse: 23.7205 - val_loss: 14.5781 - val_rmse: 23.6904\n",
      "Epoch 189/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 8.0134 - rmse: 23.6586 - val_loss: 14.0762 - val_rmse: 23.6287\n",
      "Epoch 190/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 7.7870 - rmse: 23.5970 - val_loss: 12.7665 - val_rmse: 23.5674\n",
      "Epoch 191/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 7.7719 - rmse: 23.5361 - val_loss: 12.8659 - val_rmse: 23.5066\n",
      "Epoch 192/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 7.4914 - rmse: 23.4754 - val_loss: 13.0252 - val_rmse: 23.4463\n",
      "Epoch 193/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 7.4129 - rmse: 23.4154 - val_loss: 13.0388 - val_rmse: 23.3864\n",
      "Epoch 194/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 7.3158 - rmse: 23.3558 - val_loss: 12.8486 - val_rmse: 23.3270\n",
      "Epoch 195/300\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 7.2158 - rmse: 23.2966 - val_loss: 12.6522 - val_rmse: 23.2680\n",
      "Epoch 196/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 7.1775 - rmse: 23.2378 - val_loss: 12.5299 - val_rmse: 23.2095\n",
      "Epoch 197/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 7.0752 - rmse: 23.1795 - val_loss: 12.5513 - val_rmse: 23.1514\n",
      "Epoch 198/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 6.9897 - rmse: 23.1216 - val_loss: 12.4000 - val_rmse: 23.0937\n",
      "Epoch 199/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 6.9557 - rmse: 23.0641 - val_loss: 12.2896 - val_rmse: 23.0365\n",
      "Epoch 200/300\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 6.8680 - rmse: 23.0072 - val_loss: 12.3802 - val_rmse: 22.9797\n",
      "Epoch 201/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 6.7508 - rmse: 22.9505 - val_loss: 12.3346 - val_rmse: 22.9233\n",
      "Epoch 202/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 6.6940 - rmse: 22.8944 - val_loss: 12.1353 - val_rmse: 22.8674\n",
      "Epoch 203/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 6.5705 - rmse: 22.8386 - val_loss: 12.1590 - val_rmse: 22.8118\n",
      "Epoch 204/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 6.5645 - rmse: 22.7833 - val_loss: 12.1620 - val_rmse: 22.7566\n",
      "Epoch 205/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 6.4752 - rmse: 22.7283 - val_loss: 12.3112 - val_rmse: 22.7019\n",
      "Epoch 206/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 6.4622 - rmse: 22.6738 - val_loss: 12.2619 - val_rmse: 22.6475\n",
      "Epoch 207/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 6.5206 - rmse: 22.6197 - val_loss: 12.4435 - val_rmse: 22.5936\n",
      "Epoch 208/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 6.3045 - rmse: 22.5659 - val_loss: 12.3335 - val_rmse: 22.5400\n",
      "Epoch 209/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 6.2150 - rmse: 22.5126 - val_loss: 13.3717 - val_rmse: 22.4868\n",
      "Epoch 210/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 6.2141 - rmse: 22.4595 - val_loss: 12.5751 - val_rmse: 22.4340\n",
      "Epoch 211/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 6.0685 - rmse: 22.4069 - val_loss: 12.2870 - val_rmse: 22.3816\n",
      "Epoch 212/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 6.0972 - rmse: 22.3546 - val_loss: 12.0117 - val_rmse: 22.3295\n",
      "Epoch 213/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 5.8625 - rmse: 22.3028 - val_loss: 12.8799 - val_rmse: 22.2778\n",
      "Epoch 214/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 6.0792 - rmse: 22.2512 - val_loss: 12.7827 - val_rmse: 22.2264\n",
      "Epoch 215/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 5.7767 - rmse: 22.2001 - val_loss: 12.3540 - val_rmse: 22.1754\n",
      "Epoch 216/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 5.8802 - rmse: 22.1492 - val_loss: 12.1256 - val_rmse: 22.1248\n",
      "Epoch 217/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 5.6504 - rmse: 22.0988 - val_loss: 12.4774 - val_rmse: 22.0745\n",
      "Epoch 218/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 5.6717 - rmse: 22.0487 - val_loss: 12.8690 - val_rmse: 22.0245\n",
      "Epoch 219/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 5.6806 - rmse: 21.9989 - val_loss: 11.5593 - val_rmse: 21.9749\n",
      "Epoch 220/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 5.5370 - rmse: 21.9493 - val_loss: 11.7858 - val_rmse: 21.9256\n",
      "Epoch 221/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 5.3843 - rmse: 21.9002 - val_loss: 11.7471 - val_rmse: 21.8766\n",
      "Epoch 222/300\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 5.5517 - rmse: 21.8515 - val_loss: 11.7923 - val_rmse: 21.8280\n",
      "Epoch 223/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 5.2727 - rmse: 21.8030 - val_loss: 12.9798 - val_rmse: 21.7797\n",
      "Epoch 224/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 5.4826 - rmse: 21.7549 - val_loss: 12.4616 - val_rmse: 21.7317\n",
      "Epoch 225/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 5.1817 - rmse: 21.7071 - val_loss: 11.8875 - val_rmse: 21.6840\n",
      "Epoch 226/300\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 5.2688 - rmse: 21.6595 - val_loss: 12.1843 - val_rmse: 21.6367\n",
      "Epoch 227/300\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 5.0836 - rmse: 21.6124 - val_loss: 12.9110 - val_rmse: 21.5896\n",
      "Epoch 228/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 5.0350 - rmse: 21.5655 - val_loss: 12.2721 - val_rmse: 21.5429\n",
      "Epoch 229/300\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 4.9329 - rmse: 21.5188 - val_loss: 11.8242 - val_rmse: 21.4965\n",
      "Epoch 230/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 4.9237 - rmse: 21.4726 - val_loss: 11.6661 - val_rmse: 21.4503\n",
      "Epoch 231/300\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 4.8898 - rmse: 21.4266 - val_loss: 12.4410 - val_rmse: 21.4045\n",
      "Epoch 232/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 4.8314 - rmse: 21.3810 - val_loss: 12.1403 - val_rmse: 21.3589\n",
      "Epoch 233/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 4.7234 - rmse: 21.3355 - val_loss: 12.6047 - val_rmse: 21.3137\n",
      "Epoch 234/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 4.7259 - rmse: 21.2905 - val_loss: 11.8250 - val_rmse: 21.2687\n",
      "Epoch 235/300\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 4.6545 - rmse: 21.2456 - val_loss: 11.7082 - val_rmse: 21.2240\n",
      "Epoch 236/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 4.5840 - rmse: 21.2011 - val_loss: 12.1401 - val_rmse: 21.1796\n",
      "Epoch 237/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 4.5663 - rmse: 21.1568 - val_loss: 12.3331 - val_rmse: 21.1355\n",
      "Epoch 238/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 4.4913 - rmse: 21.1128 - val_loss: 12.1340 - val_rmse: 21.0916\n",
      "Epoch 239/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 4.4845 - rmse: 21.0691 - val_loss: 11.7853 - val_rmse: 21.0480\n",
      "Epoch 240/300\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 4.4050 - rmse: 21.0256 - val_loss: 12.4720 - val_rmse: 21.0047\n",
      "Epoch 241/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 4.3614 - rmse: 20.9824 - val_loss: 12.1510 - val_rmse: 20.9617\n",
      "Epoch 242/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 4.5071 - rmse: 20.9396 - val_loss: 11.7558 - val_rmse: 20.9189\n",
      "Epoch 243/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 4.3010 - rmse: 20.8969 - val_loss: 13.0442 - val_rmse: 20.8764\n",
      "Epoch 244/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 4.3197 - rmse: 20.8546 - val_loss: 12.2286 - val_rmse: 20.8342\n",
      "Epoch 245/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 4.1857 - rmse: 20.8125 - val_loss: 11.7987 - val_rmse: 20.7922\n",
      "Epoch 246/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 4.1926 - rmse: 20.7706 - val_loss: 12.7472 - val_rmse: 20.7504\n",
      "Epoch 247/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 4.1003 - rmse: 20.7290 - val_loss: 12.5075 - val_rmse: 20.7090\n",
      "Epoch 248/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 4.0710 - rmse: 20.6876 - val_loss: 12.4358 - val_rmse: 20.6677\n",
      "Epoch 249/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 4.0711 - rmse: 20.6465 - val_loss: 12.3159 - val_rmse: 20.6267\n",
      "Epoch 250/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 4.0134 - rmse: 20.6056 - val_loss: 11.5646 - val_rmse: 20.5860\n",
      "Epoch 251/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 4.0123 - rmse: 20.5650 - val_loss: 12.1246 - val_rmse: 20.5455\n",
      "Epoch 252/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 3.9554 - rmse: 20.5247 - val_loss: 11.7358 - val_rmse: 20.5052\n",
      "Epoch 253/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 3.8896 - rmse: 20.4845 - val_loss: 12.5463 - val_rmse: 20.4652\n",
      "Epoch 254/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 3.8873 - rmse: 20.4445 - val_loss: 13.4372 - val_rmse: 20.4254\n",
      "Epoch 255/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 3.8223 - rmse: 20.4049 - val_loss: 11.6817 - val_rmse: 20.3858\n",
      "Epoch 256/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 3.8642 - rmse: 20.3655 - val_loss: 11.7616 - val_rmse: 20.3465\n",
      "Epoch 257/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 3.7625 - rmse: 20.3262 - val_loss: 12.4031 - val_rmse: 20.3074\n",
      "Epoch 258/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 3.7062 - rmse: 20.2873 - val_loss: 12.1027 - val_rmse: 20.2685\n",
      "Epoch 259/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 3.6673 - rmse: 20.2485 - val_loss: 12.6246 - val_rmse: 20.2299\n",
      "Epoch 260/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 3.6214 - rmse: 20.2100 - val_loss: 11.9996 - val_rmse: 20.1914\n",
      "Epoch 261/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 3.6248 - rmse: 20.1717 - val_loss: 12.1458 - val_rmse: 20.1532\n",
      "Epoch 262/300\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 3.5967 - rmse: 20.1336 - val_loss: 12.0318 - val_rmse: 20.1152\n",
      "Epoch 263/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 3.7411 - rmse: 20.0956 - val_loss: 13.2723 - val_rmse: 20.0775\n",
      "Epoch 264/300\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 3.4533 - rmse: 20.0580 - val_loss: 11.5428 - val_rmse: 20.0399\n",
      "Epoch 265/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 3.5931 - rmse: 20.0205 - val_loss: 11.4998 - val_rmse: 20.0025\n",
      "Epoch 266/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 3.4632 - rmse: 19.9833 - val_loss: 13.6629 - val_rmse: 19.9654\n",
      "Epoch 267/300\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 3.5033 - rmse: 19.9463 - val_loss: 13.0058 - val_rmse: 19.9285\n",
      "Epoch 268/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 3.3441 - rmse: 19.9094 - val_loss: 11.3823 - val_rmse: 19.8917\n",
      "Epoch 269/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 3.4773 - rmse: 19.8728 - val_loss: 11.6191 - val_rmse: 19.8552\n",
      "Epoch 270/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 3.3120 - rmse: 19.8364 - val_loss: 13.2691 - val_rmse: 19.8189\n",
      "Epoch 271/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 3.4037 - rmse: 19.8002 - val_loss: 12.9949 - val_rmse: 19.7828\n",
      "Epoch 272/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 3.2793 - rmse: 19.7642 - val_loss: 11.9856 - val_rmse: 19.7468\n",
      "Epoch 273/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 3.2712 - rmse: 19.7283 - val_loss: 12.2878 - val_rmse: 19.7111\n",
      "Epoch 274/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 3.2399 - rmse: 19.6927 - val_loss: 12.9355 - val_rmse: 19.6756\n",
      "Epoch 275/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 3.1792 - rmse: 19.6573 - val_loss: 12.4393 - val_rmse: 19.6402\n",
      "Epoch 276/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 3.1559 - rmse: 19.6220 - val_loss: 12.3355 - val_rmse: 19.6051\n",
      "Epoch 277/300\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 3.1419 - rmse: 19.5870 - val_loss: 11.9394 - val_rmse: 19.5701\n",
      "Epoch 278/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 3.1814 - rmse: 19.5521 - val_loss: 11.9671 - val_rmse: 19.5353\n",
      "Epoch 279/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 3.1999 - rmse: 19.5174 - val_loss: 13.3791 - val_rmse: 19.5008\n",
      "Epoch 280/300\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 3.0870 - rmse: 19.4829 - val_loss: 11.6176 - val_rmse: 19.4663\n",
      "Epoch 281/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 3.1410 - rmse: 19.4487 - val_loss: 11.6754 - val_rmse: 19.4321\n",
      "Epoch 282/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 3.0127 - rmse: 19.4144 - val_loss: 13.8741 - val_rmse: 19.3981\n",
      "Epoch 283/300\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 3.0396 - rmse: 19.3805 - val_loss: 12.3633 - val_rmse: 19.3642\n",
      "Epoch 284/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 3.0565 - rmse: 19.3468 - val_loss: 11.6977 - val_rmse: 19.3306\n",
      "Epoch 285/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 2.9734 - rmse: 19.3132 - val_loss: 12.9299 - val_rmse: 19.2971\n",
      "Epoch 286/300\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 2.9507 - rmse: 19.2798 - val_loss: 12.7489 - val_rmse: 19.2637\n",
      "Epoch 287/300\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.9228 - rmse: 19.2466 - val_loss: 12.2087 - val_rmse: 19.2306\n",
      "Epoch 288/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.9264 - rmse: 19.2135 - val_loss: 12.3895 - val_rmse: 19.1976\n",
      "Epoch 289/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.8599 - rmse: 19.1806 - val_loss: 12.0341 - val_rmse: 19.1648\n",
      "Epoch 290/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.8325 - rmse: 19.1479 - val_loss: 12.4241 - val_rmse: 19.1321\n",
      "Epoch 291/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.8125 - rmse: 19.1153 - val_loss: 12.3811 - val_rmse: 19.0996\n",
      "Epoch 292/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.7895 - rmse: 19.0829 - val_loss: 12.2918 - val_rmse: 19.0673\n",
      "Epoch 293/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.7773 - rmse: 19.0507 - val_loss: 12.7141 - val_rmse: 19.0352\n",
      "Epoch 294/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.7393 - rmse: 19.0186 - val_loss: 12.5979 - val_rmse: 19.0032\n",
      "Epoch 295/300\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.7189 - rmse: 18.9867 - val_loss: 11.8251 - val_rmse: 18.9714\n",
      "Epoch 296/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.7245 - rmse: 18.9549 - val_loss: 12.5689 - val_rmse: 18.9397\n",
      "Epoch 297/300\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 2.6801 - rmse: 18.9234 - val_loss: 11.7709 - val_rmse: 18.9082\n",
      "Epoch 298/300\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.6908 - rmse: 18.8919 - val_loss: 12.4384 - val_rmse: 18.8769\n",
      "Epoch 299/300\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.6252 - rmse: 18.8607 - val_loss: 14.1500 - val_rmse: 18.8457\n",
      "Epoch 300/300\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.6385 - rmse: 18.8296 - val_loss: 12.6733 - val_rmse: 18.8147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x238eeb6aa10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = 32\n",
    "kernel_size = 3\n",
    "dense_nodes = 512\n",
    "epochs = 300\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(dense_nodes, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer='adam', loss=tf.keras.metrics.mean_squared_error, metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "model.fit(train_input, train_output, epochs=epochs, validation_data = (test_input, test_output), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5599512009175256 0.9887563089282858 2.132794194399625 0.9981331390747293 0.5424836601307189 0.9477124183006536 0.9869281045751634\n"
     ]
    }
   ],
   "source": [
    "predicted_test_output = model.predict(test_input).flatten()\n",
    "rms = mean_squared_error(test_output, predicted_test_output, squared=False)\n",
    "r2 = r2_score(test_output, predicted_test_output)\n",
    "predicted_data_output = model.predict(data_input).flatten()\n",
    "total_rms = mean_squared_error(data_output, predicted_data_output, squared=False)\n",
    "total_r2 = r2_score(data_output, predicted_data_output)\n",
    "p1 = 0\n",
    "p5 = 0\n",
    "p10 = 0\n",
    "for i in range(predicted_data_output.size):\n",
    "    if data_output[i] != 0:\n",
    "        error = (abs(data_output[i] - predicted_data_output[i]) / predicted_data_output[i])\n",
    "        if error < 0.01:\n",
    "            p1 += 1\n",
    "        if error < 0.05:\n",
    "            p5 += 1\n",
    "        if error < 0.1:\n",
    "            p10 += 1\n",
    "p1 /= data_output.size\n",
    "p5 /= data_output.size\n",
    "p10 /= data_output.size\n",
    "print(rms, r2, total_rms, total_r2, p1, p5, p10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('result.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=' ',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow([rms, r2, total_rms, total_r2, p1, p5, p10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10713, 424, 1)\n",
      "Epoch 1/300\n",
      "335/335 [==============================] - 24s 71ms/step - loss: 0.6233 - rmse: 0.9891 - val_loss: 0.3350 - val_rmse: 0.7847\n",
      "Epoch 2/300\n",
      "335/335 [==============================] - 25s 73ms/step - loss: 0.3238 - rmse: 0.7230 - val_loss: 0.2372 - val_rmse: 0.6836\n",
      "Epoch 3/300\n",
      "335/335 [==============================] - 25s 73ms/step - loss: 0.2552 - rmse: 0.6534 - val_loss: 0.2299 - val_rmse: 0.6290\n",
      "Epoch 4/300\n",
      "335/335 [==============================] - 25s 74ms/step - loss: 0.1946 - rmse: 0.6066 - val_loss: 0.1935 - val_rmse: 0.5877\n",
      "Epoch 5/300\n",
      "335/335 [==============================] - 25s 74ms/step - loss: 0.1719 - rmse: 0.5714 - val_loss: 0.1894 - val_rmse: 0.5574\n",
      "Epoch 6/300\n",
      "335/335 [==============================] - 26s 76ms/step - loss: 0.1471 - rmse: 0.5446 - val_loss: 0.1854 - val_rmse: 0.5326\n",
      "Epoch 7/300\n",
      "335/335 [==============================] - 26s 76ms/step - loss: 0.1312 - rmse: 0.5215 - val_loss: 0.2093 - val_rmse: 0.5121\n",
      "Epoch 8/300\n",
      "335/335 [==============================] - 26s 76ms/step - loss: 0.1321 - rmse: 0.5042 - val_loss: 0.1715 - val_rmse: 0.4962\n",
      "Epoch 9/300\n",
      "335/335 [==============================] - 25s 74ms/step - loss: 0.1063 - rmse: 0.4879 - val_loss: 0.1660 - val_rmse: 0.4806\n",
      "Epoch 10/300\n",
      "335/335 [==============================] - 25s 75ms/step - loss: 0.0950 - rmse: 0.4732 - val_loss: 0.1783 - val_rmse: 0.4666\n",
      "Epoch 11/300\n",
      "335/335 [==============================] - 25s 75ms/step - loss: 0.0930 - rmse: 0.4605 - val_loss: 0.1813 - val_rmse: 0.4547\n",
      "Epoch 12/300\n",
      "335/335 [==============================] - 26s 76ms/step - loss: 0.0895 - rmse: 0.4490 - val_loss: 0.1831 - val_rmse: 0.4442\n",
      "Epoch 13/300\n",
      "335/335 [==============================] - 25s 75ms/step - loss: 0.0819 - rmse: 0.4389 - val_loss: 0.1940 - val_rmse: 0.4345\n",
      "Epoch 14/300\n",
      "335/335 [==============================] - 25s 75ms/step - loss: 0.0811 - rmse: 0.4302 - val_loss: 0.1734 - val_rmse: 0.4259\n",
      "Epoch 15/300\n",
      "335/335 [==============================] - 26s 76ms/step - loss: 0.0694 - rmse: 0.4215 - val_loss: 0.1904 - val_rmse: 0.4175\n",
      "Epoch 16/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0680 - rmse: 0.4135 - val_loss: 0.1592 - val_rmse: 0.4098\n",
      "Epoch 17/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0639 - rmse: 0.4061 - val_loss: 0.1643 - val_rmse: 0.4026\n",
      "Epoch 18/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0642 - rmse: 0.3993 - val_loss: 0.1899 - val_rmse: 0.3962\n",
      "Epoch 19/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0610 - rmse: 0.3930 - val_loss: 0.1662 - val_rmse: 0.3901\n",
      "Epoch 20/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0544 - rmse: 0.3871 - val_loss: 0.1721 - val_rmse: 0.3841\n",
      "Epoch 21/300\n",
      "335/335 [==============================] - 26s 76ms/step - loss: 0.0605 - rmse: 0.3816 - val_loss: 0.1619 - val_rmse: 0.3790\n",
      "Epoch 22/300\n",
      "335/335 [==============================] - 25s 75ms/step - loss: 0.0579 - rmse: 0.3765 - val_loss: 0.1870 - val_rmse: 0.3741\n",
      "Epoch 23/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0548 - rmse: 0.3719 - val_loss: 0.1678 - val_rmse: 0.3695\n",
      "Epoch 24/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0474 - rmse: 0.3670 - val_loss: 0.1636 - val_rmse: 0.3647\n",
      "Epoch 25/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0531 - rmse: 0.3626 - val_loss: 0.1835 - val_rmse: 0.3606\n",
      "Epoch 26/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0446 - rmse: 0.3585 - val_loss: 0.1693 - val_rmse: 0.3564\n",
      "Epoch 27/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0446 - rmse: 0.3543 - val_loss: 0.1716 - val_rmse: 0.3524\n",
      "Epoch 28/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0398 - rmse: 0.3503 - val_loss: 0.1748 - val_rmse: 0.3484\n",
      "Epoch 29/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0413 - rmse: 0.3465 - val_loss: 0.1782 - val_rmse: 0.3447\n",
      "Epoch 30/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0410 - rmse: 0.3430 - val_loss: 0.1645 - val_rmse: 0.3412\n",
      "Epoch 31/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0404 - rmse: 0.3395 - val_loss: 0.1707 - val_rmse: 0.3379\n",
      "Epoch 32/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0441 - rmse: 0.3364 - val_loss: 0.1726 - val_rmse: 0.3349\n",
      "Epoch 33/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0452 - rmse: 0.3335 - val_loss: 0.1561 - val_rmse: 0.3321\n",
      "Epoch 34/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0350 - rmse: 0.3306 - val_loss: 0.1759 - val_rmse: 0.3290\n",
      "Epoch 35/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0350 - rmse: 0.3276 - val_loss: 0.1554 - val_rmse: 0.3261\n",
      "Epoch 36/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0321 - rmse: 0.3246 - val_loss: 0.1638 - val_rmse: 0.3232\n",
      "Epoch 37/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0328 - rmse: 0.3218 - val_loss: 0.1624 - val_rmse: 0.3204\n",
      "Epoch 38/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0373 - rmse: 0.3192 - val_loss: 0.1750 - val_rmse: 0.3180\n",
      "Epoch 39/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0346 - rmse: 0.3167 - val_loss: 0.1613 - val_rmse: 0.3155\n",
      "Epoch 40/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0320 - rmse: 0.3143 - val_loss: 0.1678 - val_rmse: 0.3131\n",
      "Epoch 41/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0280 - rmse: 0.3118 - val_loss: 0.1624 - val_rmse: 0.3106\n",
      "Epoch 42/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0286 - rmse: 0.3094 - val_loss: 0.1627 - val_rmse: 0.3082\n",
      "Epoch 43/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0268 - rmse: 0.3070 - val_loss: 0.1696 - val_rmse: 0.3059\n",
      "Epoch 44/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0281 - rmse: 0.3048 - val_loss: 0.1623 - val_rmse: 0.3037\n",
      "Epoch 45/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0329 - rmse: 0.3027 - val_loss: 0.1755 - val_rmse: 0.3017\n",
      "Epoch 46/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0319 - rmse: 0.3008 - val_loss: 0.1719 - val_rmse: 0.2998\n",
      "Epoch 47/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0255 - rmse: 0.2988 - val_loss: 0.1638 - val_rmse: 0.2978\n",
      "Epoch 48/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0257 - rmse: 0.2968 - val_loss: 0.1699 - val_rmse: 0.2958\n",
      "Epoch 49/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0261 - rmse: 0.2949 - val_loss: 0.1735 - val_rmse: 0.2939\n",
      "Epoch 50/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0259 - rmse: 0.2930 - val_loss: 0.1758 - val_rmse: 0.2921\n",
      "Epoch 51/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0273 - rmse: 0.2912 - val_loss: 0.1727 - val_rmse: 0.2904\n",
      "Epoch 52/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0241 - rmse: 0.2895 - val_loss: 0.1715 - val_rmse: 0.2886\n",
      "Epoch 53/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0222 - rmse: 0.2877 - val_loss: 0.1778 - val_rmse: 0.2868\n",
      "Epoch 54/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0221 - rmse: 0.2860 - val_loss: 0.1732 - val_rmse: 0.2851\n",
      "Epoch 55/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0238 - rmse: 0.2843 - val_loss: 0.1704 - val_rmse: 0.2835\n",
      "Epoch 56/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0230 - rmse: 0.2827 - val_loss: 0.1752 - val_rmse: 0.2819\n",
      "Epoch 57/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0232 - rmse: 0.2811 - val_loss: 0.1758 - val_rmse: 0.2804\n",
      "Epoch 58/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0244 - rmse: 0.2796 - val_loss: 0.1673 - val_rmse: 0.2789\n",
      "Epoch 59/300\n",
      "335/335 [==============================] - 26s 79ms/step - loss: 0.0232 - rmse: 0.2782 - val_loss: 0.1698 - val_rmse: 0.2774\n",
      "Epoch 60/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0203 - rmse: 0.2767 - val_loss: 0.1725 - val_rmse: 0.2759\n",
      "Epoch 61/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0214 - rmse: 0.2752 - val_loss: 0.1711 - val_rmse: 0.2745\n",
      "Epoch 62/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0202 - rmse: 0.2738 - val_loss: 0.1820 - val_rmse: 0.2731\n",
      "Epoch 63/300\n",
      "335/335 [==============================] - 27s 82ms/step - loss: 0.0211 - rmse: 0.2724 - val_loss: 0.1676 - val_rmse: 0.2718\n",
      "Epoch 64/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0190 - rmse: 0.2711 - val_loss: 0.1673 - val_rmse: 0.2704\n",
      "Epoch 65/300\n",
      "335/335 [==============================] - 27s 79ms/step - loss: 0.0183 - rmse: 0.2697 - val_loss: 0.1721 - val_rmse: 0.2690\n",
      "Epoch 66/300\n",
      "335/335 [==============================] - 26s 79ms/step - loss: 0.0213 - rmse: 0.2684 - val_loss: 0.1763 - val_rmse: 0.2678\n",
      "Epoch 67/300\n",
      "335/335 [==============================] - 26s 79ms/step - loss: 0.0196 - rmse: 0.2671 - val_loss: 0.1816 - val_rmse: 0.2665\n",
      "Epoch 68/300\n",
      "335/335 [==============================] - 27s 79ms/step - loss: 0.0191 - rmse: 0.2659 - val_loss: 0.1717 - val_rmse: 0.2653\n",
      "Epoch 69/300\n",
      "335/335 [==============================] - 27s 80ms/step - loss: 0.0200 - rmse: 0.2647 - val_loss: 0.1665 - val_rmse: 0.2641\n",
      "Epoch 70/300\n",
      "335/335 [==============================] - 27s 80ms/step - loss: 0.0188 - rmse: 0.2635 - val_loss: 0.1772 - val_rmse: 0.2629\n",
      "Epoch 71/300\n",
      "335/335 [==============================] - 27s 80ms/step - loss: 0.0206 - rmse: 0.2624 - val_loss: 0.1690 - val_rmse: 0.2618\n",
      "Epoch 72/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0179 - rmse: 0.2612 - val_loss: 0.1673 - val_rmse: 0.2606\n",
      "Epoch 73/300\n",
      "335/335 [==============================] - 26s 77ms/step - loss: 0.0169 - rmse: 0.2600 - val_loss: 0.1628 - val_rmse: 0.2595\n",
      "Epoch 74/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0156 - rmse: 0.2589 - val_loss: 0.1664 - val_rmse: 0.2583\n",
      "Epoch 75/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0174 - rmse: 0.2577 - val_loss: 0.1728 - val_rmse: 0.2572\n",
      "Epoch 76/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0165 - rmse: 0.2567 - val_loss: 0.1763 - val_rmse: 0.2561\n",
      "Epoch 77/300\n",
      "335/335 [==============================] - 26s 79ms/step - loss: 0.0166 - rmse: 0.2556 - val_loss: 0.1846 - val_rmse: 0.2551\n",
      "Epoch 78/300\n",
      "335/335 [==============================] - 26s 79ms/step - loss: 0.0173 - rmse: 0.2546 - val_loss: 0.1700 - val_rmse: 0.2541\n",
      "Epoch 79/300\n",
      "335/335 [==============================] - 26s 79ms/step - loss: 0.0178 - rmse: 0.2536 - val_loss: 0.1749 - val_rmse: 0.2531\n",
      "Epoch 80/300\n",
      "335/335 [==============================] - 26s 79ms/step - loss: 0.0185 - rmse: 0.2526 - val_loss: 0.1848 - val_rmse: 0.2521\n",
      "Epoch 81/300\n",
      "335/335 [==============================] - 27s 81ms/step - loss: 0.0170 - rmse: 0.2517 - val_loss: 0.1744 - val_rmse: 0.2512\n",
      "Epoch 82/300\n",
      "335/335 [==============================] - 27s 80ms/step - loss: 0.0174 - rmse: 0.2507 - val_loss: 0.1764 - val_rmse: 0.2502\n",
      "Epoch 83/300\n",
      "335/335 [==============================] - 27s 80ms/step - loss: 0.0140 - rmse: 0.2497 - val_loss: 0.1660 - val_rmse: 0.2492\n",
      "Epoch 84/300\n",
      "335/335 [==============================] - 27s 80ms/step - loss: 0.0157 - rmse: 0.2488 - val_loss: 0.1742 - val_rmse: 0.2483\n",
      "Epoch 85/300\n",
      "335/335 [==============================] - 27s 81ms/step - loss: 0.0172 - rmse: 0.2479 - val_loss: 0.1771 - val_rmse: 0.2474\n",
      "Epoch 86/300\n",
      "335/335 [==============================] - 27s 81ms/step - loss: 0.0148 - rmse: 0.2470 - val_loss: 0.1771 - val_rmse: 0.2465\n",
      "Epoch 87/300\n",
      "335/335 [==============================] - 27s 81ms/step - loss: 0.0131 - rmse: 0.2460 - val_loss: 0.1714 - val_rmse: 0.2456\n",
      "Epoch 88/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0149 - rmse: 0.2451 - val_loss: 0.1764 - val_rmse: 0.2447\n",
      "Epoch 89/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0167 - rmse: 0.2443 - val_loss: 0.1676 - val_rmse: 0.2439\n",
      "Epoch 90/300\n",
      "335/335 [==============================] - 30s 88ms/step - loss: 0.0144 - rmse: 0.2434 - val_loss: 0.1729 - val_rmse: 0.2430\n",
      "Epoch 91/300\n",
      "335/335 [==============================] - 33s 100ms/step - loss: 0.0130 - rmse: 0.2426 - val_loss: 0.1789 - val_rmse: 0.2421\n",
      "Epoch 92/300\n",
      "335/335 [==============================] - 32s 97ms/step - loss: 0.0142 - rmse: 0.2417 - val_loss: 0.1690 - val_rmse: 0.2413\n",
      "Epoch 93/300\n",
      "335/335 [==============================] - 31s 93ms/step - loss: 0.0152 - rmse: 0.2409 - val_loss: 0.1717 - val_rmse: 0.2405\n",
      "Epoch 94/300\n",
      "335/335 [==============================] - 31s 93ms/step - loss: 0.0127 - rmse: 0.2401 - val_loss: 0.1762 - val_rmse: 0.2397\n",
      "Epoch 95/300\n",
      "335/335 [==============================] - 30s 91ms/step - loss: 0.0141 - rmse: 0.2393 - val_loss: 0.1783 - val_rmse: 0.2389\n",
      "Epoch 96/300\n",
      "335/335 [==============================] - 31s 92ms/step - loss: 0.0155 - rmse: 0.2385 - val_loss: 0.1723 - val_rmse: 0.2381\n",
      "Epoch 97/300\n",
      "335/335 [==============================] - 31s 93ms/step - loss: 0.0136 - rmse: 0.2377 - val_loss: 0.1839 - val_rmse: 0.2374\n",
      "Epoch 98/300\n",
      "335/335 [==============================] - 31s 91ms/step - loss: 0.0127 - rmse: 0.2370 - val_loss: 0.1831 - val_rmse: 0.2366\n",
      "Epoch 99/300\n",
      "335/335 [==============================] - 31s 91ms/step - loss: 0.0130 - rmse: 0.2362 - val_loss: 0.1715 - val_rmse: 0.2358\n",
      "Epoch 100/300\n",
      "335/335 [==============================] - 30s 89ms/step - loss: 0.0129 - rmse: 0.2355 - val_loss: 0.1718 - val_rmse: 0.2351\n",
      "Epoch 101/300\n",
      "335/335 [==============================] - 31s 91ms/step - loss: 0.0146 - rmse: 0.2347 - val_loss: 0.1763 - val_rmse: 0.2344\n",
      "Epoch 102/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0142 - rmse: 0.2340 - val_loss: 0.1772 - val_rmse: 0.2337\n",
      "Epoch 103/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0124 - rmse: 0.2333 - val_loss: 0.1745 - val_rmse: 0.2330\n",
      "Epoch 104/300\n",
      "335/335 [==============================] - 25s 75ms/step - loss: 0.0116 - rmse: 0.2326 - val_loss: 0.1766 - val_rmse: 0.2322\n",
      "Epoch 105/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0130 - rmse: 0.2319 - val_loss: 0.1716 - val_rmse: 0.2316\n",
      "Epoch 106/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0131 - rmse: 0.2312 - val_loss: 0.1775 - val_rmse: 0.2309\n",
      "Epoch 107/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0123 - rmse: 0.2305 - val_loss: 0.1819 - val_rmse: 0.2302\n",
      "Epoch 108/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0120 - rmse: 0.2299 - val_loss: 0.1770 - val_rmse: 0.2295\n",
      "Epoch 109/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0108 - rmse: 0.2292 - val_loss: 0.1769 - val_rmse: 0.2289\n",
      "Epoch 110/300\n",
      "335/335 [==============================] - 25s 76ms/step - loss: 0.0128 - rmse: 0.2285 - val_loss: 0.1859 - val_rmse: 0.2282\n",
      "Epoch 111/300\n",
      "335/335 [==============================] - 26s 76ms/step - loss: 0.0144 - rmse: 0.2279 - val_loss: 0.1658 - val_rmse: 0.2276\n",
      "Epoch 112/300\n",
      "335/335 [==============================] - 31s 93ms/step - loss: 0.0117 - rmse: 0.2273 - val_loss: 0.1856 - val_rmse: 0.2270\n",
      "Epoch 113/300\n",
      "335/335 [==============================] - 40s 119ms/step - loss: 0.0121 - rmse: 0.2267 - val_loss: 0.1732 - val_rmse: 0.2264\n",
      "Epoch 114/300\n",
      "335/335 [==============================] - 40s 119ms/step - loss: 0.0125 - rmse: 0.2261 - val_loss: 0.1725 - val_rmse: 0.2258\n",
      "Epoch 115/300\n",
      "335/335 [==============================] - 28s 83ms/step - loss: 0.0119 - rmse: 0.2255 - val_loss: 0.1732 - val_rmse: 0.2252\n",
      "Epoch 116/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0109 - rmse: 0.2248 - val_loss: 0.1749 - val_rmse: 0.2245\n",
      "Epoch 117/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0107 - rmse: 0.2242 - val_loss: 0.1713 - val_rmse: 0.2239\n",
      "Epoch 118/300\n",
      "335/335 [==============================] - 38s 113ms/step - loss: 0.0107 - rmse: 0.2236 - val_loss: 0.1745 - val_rmse: 0.2233\n",
      "Epoch 119/300\n",
      "335/335 [==============================] - 40s 119ms/step - loss: 0.0109 - rmse: 0.2230 - val_loss: 0.1805 - val_rmse: 0.2227\n",
      "Epoch 120/300\n",
      "335/335 [==============================] - 40s 120ms/step - loss: 0.0105 - rmse: 0.2224 - val_loss: 0.1705 - val_rmse: 0.2221\n",
      "Epoch 121/300\n",
      "335/335 [==============================] - 40s 118ms/step - loss: 0.0132 - rmse: 0.2219 - val_loss: 0.1705 - val_rmse: 0.2216\n",
      "Epoch 122/300\n",
      "335/335 [==============================] - 60s 180ms/step - loss: 0.0114 - rmse: 0.2213 - val_loss: 0.1793 - val_rmse: 0.2211\n",
      "Epoch 123/300\n",
      "335/335 [==============================] - 37s 111ms/step - loss: 0.0103 - rmse: 0.2208 - val_loss: 0.1674 - val_rmse: 0.2205\n",
      "Epoch 124/300\n",
      "335/335 [==============================] - 39s 115ms/step - loss: 0.0119 - rmse: 0.2202 - val_loss: 0.1719 - val_rmse: 0.2199\n",
      "Epoch 125/300\n",
      "335/335 [==============================] - 38s 112ms/step - loss: 0.0124 - rmse: 0.2197 - val_loss: 0.1691 - val_rmse: 0.2194\n",
      "Epoch 126/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0099 - rmse: 0.2191 - val_loss: 0.1695 - val_rmse: 0.2189\n",
      "Epoch 127/300\n",
      "335/335 [==============================] - 37s 111ms/step - loss: 0.0098 - rmse: 0.2186 - val_loss: 0.1774 - val_rmse: 0.2183\n",
      "Epoch 128/300\n",
      "335/335 [==============================] - 38s 115ms/step - loss: 0.0102 - rmse: 0.2181 - val_loss: 0.1707 - val_rmse: 0.2178\n",
      "Epoch 129/300\n",
      "335/335 [==============================] - 36s 107ms/step - loss: 0.0103 - rmse: 0.2175 - val_loss: 0.1721 - val_rmse: 0.2173\n",
      "Epoch 130/300\n",
      "335/335 [==============================] - 38s 114ms/step - loss: 0.0107 - rmse: 0.2170 - val_loss: 0.1748 - val_rmse: 0.2167\n",
      "Epoch 131/300\n",
      "335/335 [==============================] - 38s 115ms/step - loss: 0.0109 - rmse: 0.2165 - val_loss: 0.1753 - val_rmse: 0.2162\n",
      "Epoch 132/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0101 - rmse: 0.2160 - val_loss: 0.1801 - val_rmse: 0.2157\n",
      "Epoch 133/300\n",
      "335/335 [==============================] - 38s 114ms/step - loss: 0.0109 - rmse: 0.2155 - val_loss: 0.1736 - val_rmse: 0.2152\n",
      "Epoch 134/300\n",
      "335/335 [==============================] - 38s 114ms/step - loss: 0.0106 - rmse: 0.2150 - val_loss: 0.1763 - val_rmse: 0.2148\n",
      "Epoch 135/300\n",
      "335/335 [==============================] - 39s 117ms/step - loss: 0.0095 - rmse: 0.2145 - val_loss: 0.1698 - val_rmse: 0.2143\n",
      "Epoch 136/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0087 - rmse: 0.2140 - val_loss: 0.1785 - val_rmse: 0.2138\n",
      "Epoch 137/300\n",
      "335/335 [==============================] - 39s 117ms/step - loss: 0.0091 - rmse: 0.2135 - val_loss: 0.1766 - val_rmse: 0.2133\n",
      "Epoch 138/300\n",
      "335/335 [==============================] - 37s 110ms/step - loss: 0.0116 - rmse: 0.2130 - val_loss: 0.1777 - val_rmse: 0.2128\n",
      "Epoch 139/300\n",
      "335/335 [==============================] - 38s 113ms/step - loss: 0.0097 - rmse: 0.2126 - val_loss: 0.1726 - val_rmse: 0.2123\n",
      "Epoch 140/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0090 - rmse: 0.2121 - val_loss: 0.1823 - val_rmse: 0.2119\n",
      "Epoch 141/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0091 - rmse: 0.2116 - val_loss: 0.1699 - val_rmse: 0.2114\n",
      "Epoch 142/300\n",
      "335/335 [==============================] - 38s 114ms/step - loss: 0.0084 - rmse: 0.2112 - val_loss: 0.1686 - val_rmse: 0.2109\n",
      "Epoch 143/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0091 - rmse: 0.2107 - val_loss: 0.1670 - val_rmse: 0.2105\n",
      "Epoch 144/300\n",
      "335/335 [==============================] - 38s 113ms/step - loss: 0.0099 - rmse: 0.2102 - val_loss: 0.1758 - val_rmse: 0.2100\n",
      "Epoch 145/300\n",
      "335/335 [==============================] - 39s 115ms/step - loss: 0.0107 - rmse: 0.2098 - val_loss: 0.1834 - val_rmse: 0.2096\n",
      "Epoch 146/300\n",
      "335/335 [==============================] - 39s 115ms/step - loss: 0.0102 - rmse: 0.2094 - val_loss: 0.1741 - val_rmse: 0.2092\n",
      "Epoch 147/300\n",
      "335/335 [==============================] - 39s 115ms/step - loss: 0.0094 - rmse: 0.2089 - val_loss: 0.1737 - val_rmse: 0.2087\n",
      "Epoch 148/300\n",
      "335/335 [==============================] - 38s 113ms/step - loss: 0.0092 - rmse: 0.2085 - val_loss: 0.1726 - val_rmse: 0.2083\n",
      "Epoch 149/300\n",
      "335/335 [==============================] - 33s 98ms/step - loss: 0.0084 - rmse: 0.2081 - val_loss: 0.1787 - val_rmse: 0.2079\n",
      "Epoch 150/300\n",
      "335/335 [==============================] - 36s 107ms/step - loss: 0.0093 - rmse: 0.2076 - val_loss: 0.1724 - val_rmse: 0.2074\n",
      "Epoch 151/300\n",
      "335/335 [==============================] - 38s 114ms/step - loss: 0.0089 - rmse: 0.2072 - val_loss: 0.1718 - val_rmse: 0.2070\n",
      "Epoch 152/300\n",
      "335/335 [==============================] - 38s 112ms/step - loss: 0.0087 - rmse: 0.2068 - val_loss: 0.1807 - val_rmse: 0.2066\n",
      "Epoch 153/300\n",
      "335/335 [==============================] - 38s 114ms/step - loss: 0.0086 - rmse: 0.2064 - val_loss: 0.1762 - val_rmse: 0.2062\n",
      "Epoch 154/300\n",
      "335/335 [==============================] - 37s 110ms/step - loss: 0.0090 - rmse: 0.2060 - val_loss: 0.1834 - val_rmse: 0.2058\n",
      "Epoch 155/300\n",
      "335/335 [==============================] - 44s 130ms/step - loss: 0.0092 - rmse: 0.2056 - val_loss: 0.1709 - val_rmse: 0.2054\n",
      "Epoch 156/300\n",
      "335/335 [==============================] - 38s 112ms/step - loss: 0.0083 - rmse: 0.2052 - val_loss: 0.1763 - val_rmse: 0.2050\n",
      "Epoch 157/300\n",
      "335/335 [==============================] - 39s 116ms/step - loss: 0.0089 - rmse: 0.2048 - val_loss: 0.1794 - val_rmse: 0.2046\n",
      "Epoch 158/300\n",
      "335/335 [==============================] - 39s 117ms/step - loss: 0.0085 - rmse: 0.2044 - val_loss: 0.1813 - val_rmse: 0.2042\n",
      "Epoch 159/300\n",
      "335/335 [==============================] - 38s 113ms/step - loss: 0.0084 - rmse: 0.2040 - val_loss: 0.1779 - val_rmse: 0.2038\n",
      "Epoch 160/300\n",
      "335/335 [==============================] - 38s 112ms/step - loss: 0.0082 - rmse: 0.2036 - val_loss: 0.1741 - val_rmse: 0.2034\n",
      "Epoch 161/300\n",
      "335/335 [==============================] - 39s 118ms/step - loss: 0.0082 - rmse: 0.2032 - val_loss: 0.1722 - val_rmse: 0.2030\n",
      "Epoch 162/300\n",
      "335/335 [==============================] - 33s 98ms/step - loss: 0.0085 - rmse: 0.2028 - val_loss: 0.1817 - val_rmse: 0.2026\n",
      "Epoch 163/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0090 - rmse: 0.2024 - val_loss: 0.1756 - val_rmse: 0.2023\n",
      "Epoch 164/300\n",
      "335/335 [==============================] - 27s 80ms/step - loss: 0.0081 - rmse: 0.2021 - val_loss: 0.1706 - val_rmse: 0.2019\n",
      "Epoch 165/300\n",
      "335/335 [==============================] - 27s 79ms/step - loss: 0.0087 - rmse: 0.2017 - val_loss: 0.1724 - val_rmse: 0.2015\n",
      "Epoch 166/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0081 - rmse: 0.2013 - val_loss: 0.1766 - val_rmse: 0.2012\n",
      "Epoch 167/300\n",
      "335/335 [==============================] - 258s 772ms/step - loss: 0.0075 - rmse: 0.2010 - val_loss: 0.1760 - val_rmse: 0.2008\n",
      "Epoch 168/300\n",
      "335/335 [==============================] - 37s 111ms/step - loss: 0.0072 - rmse: 0.2006 - val_loss: 0.1787 - val_rmse: 0.2004\n",
      "Epoch 169/300\n",
      "335/335 [==============================] - 35s 104ms/step - loss: 0.0106 - rmse: 0.2002 - val_loss: 0.1770 - val_rmse: 0.2001\n",
      "Epoch 170/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0077 - rmse: 0.1999 - val_loss: 0.1821 - val_rmse: 0.1997\n",
      "Epoch 171/300\n",
      "335/335 [==============================] - 28s 84ms/step - loss: 0.0081 - rmse: 0.1995 - val_loss: 0.1762 - val_rmse: 0.1994\n",
      "Epoch 172/300\n",
      "335/335 [==============================] - 28s 83ms/step - loss: 0.0081 - rmse: 0.1992 - val_loss: 0.1791 - val_rmse: 0.1990\n",
      "Epoch 173/300\n",
      "335/335 [==============================] - 27s 81ms/step - loss: 0.0080 - rmse: 0.1989 - val_loss: 0.1694 - val_rmse: 0.1987\n",
      "Epoch 174/300\n",
      "335/335 [==============================] - 30s 90ms/step - loss: 0.0080 - rmse: 0.1985 - val_loss: 0.1767 - val_rmse: 0.1983\n",
      "Epoch 175/300\n",
      "335/335 [==============================] - 33s 99ms/step - loss: 0.0076 - rmse: 0.1982 - val_loss: 0.1720 - val_rmse: 0.1980\n",
      "Epoch 176/300\n",
      "335/335 [==============================] - 33s 97ms/step - loss: 0.0078 - rmse: 0.1978 - val_loss: 0.1756 - val_rmse: 0.1976\n",
      "Epoch 177/300\n",
      "335/335 [==============================] - 30s 89ms/step - loss: 0.0080 - rmse: 0.1975 - val_loss: 0.1786 - val_rmse: 0.1973\n",
      "Epoch 178/300\n",
      "335/335 [==============================] - 30s 91ms/step - loss: 0.0078 - rmse: 0.1972 - val_loss: 0.1746 - val_rmse: 0.1970\n",
      "Epoch 179/300\n",
      "335/335 [==============================] - 30s 88ms/step - loss: 0.0075 - rmse: 0.1968 - val_loss: 0.1778 - val_rmse: 0.1967\n",
      "Epoch 180/300\n",
      "335/335 [==============================] - 27s 79ms/step - loss: 0.0071 - rmse: 0.1965 - val_loss: 0.1767 - val_rmse: 0.1963\n",
      "Epoch 181/300\n",
      "335/335 [==============================] - 28s 84ms/step - loss: 0.0077 - rmse: 0.1962 - val_loss: 0.1806 - val_rmse: 0.1960\n",
      "Epoch 182/300\n",
      "335/335 [==============================] - 27s 82ms/step - loss: 0.0076 - rmse: 0.1958 - val_loss: 0.1769 - val_rmse: 0.1957\n",
      "Epoch 183/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0072 - rmse: 0.1955 - val_loss: 0.1703 - val_rmse: 0.1954\n",
      "Epoch 184/300\n",
      "335/335 [==============================] - 27s 79ms/step - loss: 0.0071 - rmse: 0.1952 - val_loss: 0.1881 - val_rmse: 0.1950\n",
      "Epoch 185/300\n",
      "335/335 [==============================] - 27s 81ms/step - loss: 0.0077 - rmse: 0.1949 - val_loss: 0.1792 - val_rmse: 0.1947\n",
      "Epoch 186/300\n",
      "335/335 [==============================] - 27s 80ms/step - loss: 0.0073 - rmse: 0.1946 - val_loss: 0.1772 - val_rmse: 0.1944\n",
      "Epoch 187/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0075 - rmse: 0.1943 - val_loss: 0.1789 - val_rmse: 0.1941\n",
      "Epoch 188/300\n",
      "335/335 [==============================] - 29s 87ms/step - loss: 0.0082 - rmse: 0.1940 - val_loss: 0.1768 - val_rmse: 0.1938\n",
      "Epoch 189/300\n",
      "335/335 [==============================] - 30s 88ms/step - loss: 0.0070 - rmse: 0.1936 - val_loss: 0.1857 - val_rmse: 0.1935\n",
      "Epoch 190/300\n",
      "335/335 [==============================] - 29s 87ms/step - loss: 0.0075 - rmse: 0.1933 - val_loss: 0.1770 - val_rmse: 0.1932\n",
      "Epoch 191/300\n",
      "335/335 [==============================] - 36s 108ms/step - loss: 0.0067 - rmse: 0.1930 - val_loss: 0.1753 - val_rmse: 0.1929\n",
      "Epoch 192/300\n",
      "335/335 [==============================] - 37s 110ms/step - loss: 0.0064 - rmse: 0.1927 - val_loss: 0.1766 - val_rmse: 0.1926\n",
      "Epoch 193/300\n",
      "335/335 [==============================] - 32s 96ms/step - loss: 0.0074 - rmse: 0.1924 - val_loss: 0.1759 - val_rmse: 0.1923\n",
      "Epoch 194/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0076 - rmse: 0.1921 - val_loss: 0.1774 - val_rmse: 0.1920\n",
      "Epoch 195/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0072 - rmse: 0.1919 - val_loss: 0.1757 - val_rmse: 0.1917\n",
      "Epoch 196/300\n",
      "335/335 [==============================] - 897s 3s/step - loss: 0.0074 - rmse: 0.1916 - val_loss: 0.1742 - val_rmse: 0.1914\n",
      "Epoch 197/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0082 - rmse: 0.1913 - val_loss: 0.1720 - val_rmse: 0.1911\n",
      "Epoch 198/300\n",
      "335/335 [==============================] - 24s 72ms/step - loss: 0.0069 - rmse: 0.1910 - val_loss: 0.1778 - val_rmse: 0.1909\n",
      "Epoch 199/300\n",
      "335/335 [==============================] - 26s 78ms/step - loss: 0.0067 - rmse: 0.1907 - val_loss: 0.1746 - val_rmse: 0.1906\n",
      "Epoch 200/300\n",
      "335/335 [==============================] - 31s 91ms/step - loss: 0.0077 - rmse: 0.1904 - val_loss: 0.1802 - val_rmse: 0.1903\n",
      "Epoch 201/300\n",
      "335/335 [==============================] - 31s 93ms/step - loss: 0.0071 - rmse: 0.1902 - val_loss: 0.1771 - val_rmse: 0.1900\n",
      "Epoch 202/300\n",
      "335/335 [==============================] - 30s 90ms/step - loss: 0.0076 - rmse: 0.1899 - val_loss: 0.1832 - val_rmse: 0.1898\n",
      "Epoch 203/300\n",
      "335/335 [==============================] - 30s 91ms/step - loss: 0.0069 - rmse: 0.1896 - val_loss: 0.1801 - val_rmse: 0.1895\n",
      "Epoch 204/300\n",
      "335/335 [==============================] - 31s 91ms/step - loss: 0.0068 - rmse: 0.1893 - val_loss: 0.1770 - val_rmse: 0.1892\n",
      "Epoch 205/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0073 - rmse: 0.1891 - val_loss: 0.1781 - val_rmse: 0.1889\n",
      "Epoch 206/300\n",
      "335/335 [==============================] - 27s 82ms/step - loss: 0.0070 - rmse: 0.1888 - val_loss: 0.1759 - val_rmse: 0.1887\n",
      "Epoch 207/300\n",
      "335/335 [==============================] - 27s 82ms/step - loss: 0.0074 - rmse: 0.1886 - val_loss: 0.1731 - val_rmse: 0.1884\n",
      "Epoch 208/300\n",
      "335/335 [==============================] - 27s 82ms/step - loss: 0.0064 - rmse: 0.1883 - val_loss: 0.1750 - val_rmse: 0.1881\n",
      "Epoch 209/300\n",
      "335/335 [==============================] - 28s 83ms/step - loss: 0.0064 - rmse: 0.1880 - val_loss: 0.1755 - val_rmse: 0.1879\n",
      "Epoch 210/300\n",
      "335/335 [==============================] - 28s 84ms/step - loss: 0.0067 - rmse: 0.1877 - val_loss: 0.1780 - val_rmse: 0.1876\n",
      "Epoch 211/300\n",
      "335/335 [==============================] - 28s 84ms/step - loss: 0.0071 - rmse: 0.1875 - val_loss: 0.1763 - val_rmse: 0.1874\n",
      "Epoch 212/300\n",
      "335/335 [==============================] - 28s 82ms/step - loss: 0.0068 - rmse: 0.1872 - val_loss: 0.1819 - val_rmse: 0.1871\n",
      "Epoch 213/300\n",
      "335/335 [==============================] - 28s 84ms/step - loss: 0.0067 - rmse: 0.1870 - val_loss: 0.1728 - val_rmse: 0.1868\n",
      "Epoch 214/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0067 - rmse: 0.1867 - val_loss: 0.1798 - val_rmse: 0.1866\n",
      "Epoch 215/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0075 - rmse: 0.1865 - val_loss: 0.1792 - val_rmse: 0.1864\n",
      "Epoch 216/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0073 - rmse: 0.1862 - val_loss: 0.1747 - val_rmse: 0.1861\n",
      "Epoch 217/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0068 - rmse: 0.1860 - val_loss: 0.1719 - val_rmse: 0.1859\n",
      "Epoch 218/300\n",
      "335/335 [==============================] - 30s 89ms/step - loss: 0.0060 - rmse: 0.1857 - val_loss: 0.1714 - val_rmse: 0.1856\n",
      "Epoch 219/300\n",
      "335/335 [==============================] - 29s 88ms/step - loss: 0.0061 - rmse: 0.1855 - val_loss: 0.1787 - val_rmse: 0.1853\n",
      "Epoch 220/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0064 - rmse: 0.1852 - val_loss: 0.1811 - val_rmse: 0.1851\n",
      "Epoch 221/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0063 - rmse: 0.1850 - val_loss: 0.1677 - val_rmse: 0.1849\n",
      "Epoch 222/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0074 - rmse: 0.1847 - val_loss: 0.1781 - val_rmse: 0.1846\n",
      "Epoch 223/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0081 - rmse: 0.1845 - val_loss: 0.1708 - val_rmse: 0.1844\n",
      "Epoch 224/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0064 - rmse: 0.1843 - val_loss: 0.1757 - val_rmse: 0.1842\n",
      "Epoch 225/300\n",
      "335/335 [==============================] - 30s 90ms/step - loss: 0.0068 - rmse: 0.1840 - val_loss: 0.1755 - val_rmse: 0.1839\n",
      "Epoch 226/300\n",
      "335/335 [==============================] - 31s 93ms/step - loss: 0.0054 - rmse: 0.1838 - val_loss: 0.1752 - val_rmse: 0.1837\n",
      "Epoch 227/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0068 - rmse: 0.1836 - val_loss: 0.1806 - val_rmse: 0.1835\n",
      "Epoch 228/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0065 - rmse: 0.1833 - val_loss: 0.1807 - val_rmse: 0.1832\n",
      "Epoch 229/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0062 - rmse: 0.1831 - val_loss: 0.1779 - val_rmse: 0.1830\n",
      "Epoch 230/300\n",
      "335/335 [==============================] - 34s 101ms/step - loss: 0.0067 - rmse: 0.1829 - val_loss: 0.1792 - val_rmse: 0.1828\n",
      "Epoch 231/300\n",
      "335/335 [==============================] - 41s 123ms/step - loss: 0.0064 - rmse: 0.1827 - val_loss: 0.1784 - val_rmse: 0.1826\n",
      "Epoch 232/300\n",
      "335/335 [==============================] - 44s 131ms/step - loss: 0.0065 - rmse: 0.1824 - val_loss: 0.1793 - val_rmse: 0.1823\n",
      "Epoch 233/300\n",
      "335/335 [==============================] - 42s 125ms/step - loss: 0.0057 - rmse: 0.1822 - val_loss: 0.1744 - val_rmse: 0.1821\n",
      "Epoch 234/300\n",
      "335/335 [==============================] - 43s 127ms/step - loss: 0.0061 - rmse: 0.1820 - val_loss: 0.1762 - val_rmse: 0.1819\n",
      "Epoch 235/300\n",
      "335/335 [==============================] - 43s 129ms/step - loss: 0.0063 - rmse: 0.1818 - val_loss: 0.1843 - val_rmse: 0.1817\n",
      "Epoch 236/300\n",
      "335/335 [==============================] - 41s 123ms/step - loss: 0.0073 - rmse: 0.1816 - val_loss: 0.1815 - val_rmse: 0.1815\n",
      "Epoch 237/300\n",
      "335/335 [==============================] - 40s 120ms/step - loss: 0.0062 - rmse: 0.1813 - val_loss: 0.1733 - val_rmse: 0.1812\n",
      "Epoch 238/300\n",
      "335/335 [==============================] - 43s 127ms/step - loss: 0.0064 - rmse: 0.1811 - val_loss: 0.1798 - val_rmse: 0.1810\n",
      "Epoch 239/300\n",
      "335/335 [==============================] - 31s 91ms/step - loss: 0.0070 - rmse: 0.1809 - val_loss: 0.1786 - val_rmse: 0.1808\n",
      "Epoch 240/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0059 - rmse: 0.1807 - val_loss: 0.1762 - val_rmse: 0.1806\n",
      "Epoch 241/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0060 - rmse: 0.1805 - val_loss: 0.1803 - val_rmse: 0.1804\n",
      "Epoch 242/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0066 - rmse: 0.1803 - val_loss: 0.1796 - val_rmse: 0.1802\n",
      "Epoch 243/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0060 - rmse: 0.1801 - val_loss: 0.1757 - val_rmse: 0.1800\n",
      "Epoch 244/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0059 - rmse: 0.1799 - val_loss: 0.1825 - val_rmse: 0.1798\n",
      "Epoch 245/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0062 - rmse: 0.1797 - val_loss: 0.1732 - val_rmse: 0.1795\n",
      "Epoch 246/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0067 - rmse: 0.1795 - val_loss: 0.1776 - val_rmse: 0.1793\n",
      "Epoch 247/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0055 - rmse: 0.1792 - val_loss: 0.1824 - val_rmse: 0.1791\n",
      "Epoch 248/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0055 - rmse: 0.1790 - val_loss: 0.1806 - val_rmse: 0.1789\n",
      "Epoch 249/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0055 - rmse: 0.1788 - val_loss: 0.1763 - val_rmse: 0.1787\n",
      "Epoch 250/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0052 - rmse: 0.1786 - val_loss: 0.1768 - val_rmse: 0.1785\n",
      "Epoch 251/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0064 - rmse: 0.1784 - val_loss: 0.1786 - val_rmse: 0.1783\n",
      "Epoch 252/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0073 - rmse: 0.1782 - val_loss: 0.1753 - val_rmse: 0.1781\n",
      "Epoch 253/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0063 - rmse: 0.1780 - val_loss: 0.1848 - val_rmse: 0.1779\n",
      "Epoch 254/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0057 - rmse: 0.1778 - val_loss: 0.1742 - val_rmse: 0.1777\n",
      "Epoch 255/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0056 - rmse: 0.1776 - val_loss: 0.1734 - val_rmse: 0.1775\n",
      "Epoch 256/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0051 - rmse: 0.1774 - val_loss: 0.1703 - val_rmse: 0.1773\n",
      "Epoch 257/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0058 - rmse: 0.1772 - val_loss: 0.1771 - val_rmse: 0.1771\n",
      "Epoch 258/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0054 - rmse: 0.1770 - val_loss: 0.1780 - val_rmse: 0.1769\n",
      "Epoch 259/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0054 - rmse: 0.1768 - val_loss: 0.1874 - val_rmse: 0.1768\n",
      "Epoch 260/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0062 - rmse: 0.1767 - val_loss: 0.1779 - val_rmse: 0.1766\n",
      "Epoch 261/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0057 - rmse: 0.1765 - val_loss: 0.1754 - val_rmse: 0.1764\n",
      "Epoch 262/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0054 - rmse: 0.1763 - val_loss: 0.1770 - val_rmse: 0.1762\n",
      "Epoch 263/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0059 - rmse: 0.1761 - val_loss: 0.1760 - val_rmse: 0.1760\n",
      "Epoch 264/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0060 - rmse: 0.1759 - val_loss: 0.1796 - val_rmse: 0.1758\n",
      "Epoch 265/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0051 - rmse: 0.1757 - val_loss: 0.1778 - val_rmse: 0.1756\n",
      "Epoch 266/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0054 - rmse: 0.1755 - val_loss: 0.1765 - val_rmse: 0.1754\n",
      "Epoch 267/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0056 - rmse: 0.1754 - val_loss: 0.1772 - val_rmse: 0.1753\n",
      "Epoch 268/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0055 - rmse: 0.1752 - val_loss: 0.1792 - val_rmse: 0.1751\n",
      "Epoch 269/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0060 - rmse: 0.1750 - val_loss: 0.1784 - val_rmse: 0.1749\n",
      "Epoch 270/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0053 - rmse: 0.1748 - val_loss: 0.1855 - val_rmse: 0.1747\n",
      "Epoch 271/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0057 - rmse: 0.1746 - val_loss: 0.1769 - val_rmse: 0.1745\n",
      "Epoch 272/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0057 - rmse: 0.1745 - val_loss: 0.1800 - val_rmse: 0.1744\n",
      "Epoch 273/300\n",
      "335/335 [==============================] - 29s 87ms/step - loss: 0.0063 - rmse: 0.1743 - val_loss: 0.1801 - val_rmse: 0.1742\n",
      "Epoch 274/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0058 - rmse: 0.1741 - val_loss: 0.1746 - val_rmse: 0.1740\n",
      "Epoch 275/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0049 - rmse: 0.1739 - val_loss: 0.1756 - val_rmse: 0.1738\n",
      "Epoch 276/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0055 - rmse: 0.1738 - val_loss: 0.1759 - val_rmse: 0.1737\n",
      "Epoch 277/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0054 - rmse: 0.1736 - val_loss: 0.1807 - val_rmse: 0.1735\n",
      "Epoch 278/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0052 - rmse: 0.1734 - val_loss: 0.1746 - val_rmse: 0.1733\n",
      "Epoch 279/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0047 - rmse: 0.1732 - val_loss: 0.1741 - val_rmse: 0.1731\n",
      "Epoch 280/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0051 - rmse: 0.1730 - val_loss: 0.1860 - val_rmse: 0.1730\n",
      "Epoch 281/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0063 - rmse: 0.1729 - val_loss: 0.1804 - val_rmse: 0.1728\n",
      "Epoch 282/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0056 - rmse: 0.1727 - val_loss: 0.1794 - val_rmse: 0.1726\n",
      "Epoch 283/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0055 - rmse: 0.1726 - val_loss: 0.1776 - val_rmse: 0.1725\n",
      "Epoch 284/300\n",
      "335/335 [==============================] - 28s 84ms/step - loss: 0.0062 - rmse: 0.1724 - val_loss: 0.1809 - val_rmse: 0.1723\n",
      "Epoch 285/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0055 - rmse: 0.1722 - val_loss: 0.1784 - val_rmse: 0.1722\n",
      "Epoch 286/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0056 - rmse: 0.1721 - val_loss: 0.1782 - val_rmse: 0.1720\n",
      "Epoch 287/300\n",
      "335/335 [==============================] - 30s 89ms/step - loss: 0.0047 - rmse: 0.1719 - val_loss: 0.1751 - val_rmse: 0.1718\n",
      "Epoch 288/300\n",
      "335/335 [==============================] - 29s 86ms/step - loss: 0.0048 - rmse: 0.1717 - val_loss: 0.1784 - val_rmse: 0.1716\n",
      "Epoch 289/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0053 - rmse: 0.1716 - val_loss: 0.1772 - val_rmse: 0.1715\n",
      "Epoch 290/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0046 - rmse: 0.1714 - val_loss: 0.1792 - val_rmse: 0.1713\n",
      "Epoch 291/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0053 - rmse: 0.1712 - val_loss: 0.1777 - val_rmse: 0.1712\n",
      "Epoch 292/300\n",
      "335/335 [==============================] - 29s 85ms/step - loss: 0.0061 - rmse: 0.1711 - val_loss: 0.1843 - val_rmse: 0.1710\n",
      "Epoch 293/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0064 - rmse: 0.1709 - val_loss: 0.1801 - val_rmse: 0.1709\n",
      "Epoch 294/300\n",
      "335/335 [==============================] - 28s 85ms/step - loss: 0.0055 - rmse: 0.1708 - val_loss: 0.1747 - val_rmse: 0.1707\n",
      "Epoch 295/300\n",
      "335/335 [==============================] - 30s 89ms/step - loss: 0.0047 - rmse: 0.1706 - val_loss: 0.1799 - val_rmse: 0.1705\n",
      "Epoch 296/300\n",
      "335/335 [==============================] - 29s 88ms/step - loss: 0.0052 - rmse: 0.1705 - val_loss: 0.1768 - val_rmse: 0.1704\n",
      "Epoch 297/300\n",
      "335/335 [==============================] - 34s 103ms/step - loss: 0.0052 - rmse: 0.1703 - val_loss: 0.1798 - val_rmse: 0.1702\n",
      "Epoch 298/300\n",
      "335/335 [==============================] - 29s 88ms/step - loss: 0.0062 - rmse: 0.1702 - val_loss: 0.1788 - val_rmse: 0.1701\n",
      "Epoch 299/300\n",
      "335/335 [==============================] - 30s 89ms/step - loss: 0.0047 - rmse: 0.1700 - val_loss: 0.1763 - val_rmse: 0.1699\n",
      "Epoch 300/300\n",
      "335/335 [==============================] - 30s 88ms/step - loss: 0.0046 - rmse: 0.1698 - val_loss: 0.1804 - val_rmse: 0.1698\n",
      "0.424697307962814 0.9392159017447382 0.11265573921535571 0.9958456854387802 0.4220363118547526 0.8375756496974012 0.9196333214667142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.python.keras import layers, models\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "save_path = \"C:/Users/13764/Documents/academics/zhejiang/GP-WP/GP-WP data/dataset_14_Logp.npy\"\n",
    "temp = np.load(save_path, allow_pickle = True)\n",
    "res_dict_dataset = temp.item()\n",
    "data_output = res_dict_dataset[\"data_output\"]\n",
    "train_output = res_dict_dataset[\"train_output\"]\n",
    "test_output = res_dict_dataset[\"test_output\"]\n",
    "data_input = res_dict_dataset[\"data_input\"]\n",
    "train_input = res_dict_dataset[\"train_input\"]\n",
    "test_input = res_dict_dataset[\"test_input\"]\n",
    "\n",
    "# adds a dummy dimension for cnn\n",
    "train_input = train_input[..., None]\n",
    "test_input = test_input[...,None]\n",
    "data_input = data_input[..., None]\n",
    "print(train_input.shape)\n",
    "\n",
    "# parameters\n",
    "filters = 64\n",
    "kernel_size = 3\n",
    "dense_nodes = 512\n",
    "epochs = 300\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# cnn model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(dense_nodes, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer='adam', loss=tf.keras.metrics.mean_squared_error, metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n",
    "model.fit(train_input, train_output, epochs=epochs, validation_data = (test_input, test_output), callbacks=[tensorboard_callback])\n",
    "\n",
    "# calculate errors\n",
    "predicted_test_output = model.predict(test_input).flatten()\n",
    "rms = mean_squared_error(test_output, predicted_test_output, squared=False)\n",
    "r2 = r2_score(test_output, predicted_test_output)\n",
    "predicted_data_output = model.predict(data_input).flatten()\n",
    "total_rms = mean_squared_error(data_output, predicted_data_output, squared=False)\n",
    "total_r2 = r2_score(data_output, predicted_data_output)\n",
    "p1 = 0\n",
    "p5 = 0\n",
    "p10 = 0\n",
    "for i in range(predicted_data_output.size):\n",
    "    if data_output[i] != 0:\n",
    "        error = (abs(data_output[i] - predicted_data_output[i]) / predicted_data_output[i])\n",
    "        if error < 0.01:\n",
    "            p1 += 1\n",
    "        if error < 0.05:\n",
    "            p5 += 1\n",
    "        if error < 0.1:\n",
    "            p10 += 1\n",
    "p1 /= data_output.size\n",
    "p5 /= data_output.size\n",
    "p10 /= data_output.size\n",
    "print(rms, r2, total_rms, total_r2, p1, p5, p10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RootMeanSquaredError' object has no attribute '_serialized_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers, models\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mC:/Users/13764/Documents/academics/zhejiang/GP-WP/GP-WP data/models\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mLogp\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\keras\\saving\\save.py:205\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    203\u001b[0m       filepath \u001b[39m=\u001b[39m path_to_string(filepath)\n\u001b[0;32m    204\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39;49mload(filepath, \u001b[39mcompile\u001b[39;49m, options)\n\u001b[0;32m    207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mUnable to load model. Filepath is not an hdf5 file (or h5py is not \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    209\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mavailable) or SavedModel.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py:153\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, compile, options)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mfor\u001b[39;00m node_id, loaded_node \u001b[39min\u001b[39;00m keras_loader\u001b[39m.\u001b[39mloaded_nodes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    152\u001b[0m   nodes_to_load[keras_loader\u001b[39m.\u001b[39mget_path(node_id)] \u001b[39m=\u001b[39m loaded_node\n\u001b[1;32m--> 153\u001b[0m loaded \u001b[39m=\u001b[39m tf_load\u001b[39m.\u001b[39;49mload_partial(path, nodes_to_load, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    155\u001b[0m \u001b[39m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m keras_loader\u001b[39m.\u001b[39mfinalize_objects()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\saved_model\\load.py:966\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39minit_scope():\n\u001b[0;32m    965\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 966\u001b[0m     loader \u001b[39m=\u001b[39m Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m    967\u001b[0m                     ckpt_options, options, filters)\n\u001b[0;32m    968\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mNotFoundError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    969\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    970\u001b[0m         \u001b[39mstr\u001b[39m(err) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m You may be trying to load on a different device \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfrom the computational device. Consider setting the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    972\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`experimental_io_device` option in `tf.saved_model.LoadOptions` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    973\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto the io_device such as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m/job:localhost\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\saved_model\\load.py:192\u001b[0m, in \u001b[0;36mLoader.__init__\u001b[1;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39m# Order all nodes or filtered nodes using the dependencies.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ordered_node_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_ordered_node_ids()\n\u001b[1;32m--> 192\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_all()\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_options\u001b[39m.\u001b[39mexperimental_skip_checkpoint:\n\u001b[0;32m    195\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restore_checkpoint()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\saved_model\\load.py:290\u001b[0m, in \u001b[0;36mLoader._load_all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads all nodes and functions from the SavedModel and their edges.\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_nodes()\n\u001b[1;32m--> 290\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_edges()\n\u001b[0;32m    292\u001b[0m \u001b[39m# Set up concrete functions that aren't part of the object graph\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m# (e.g. gradient functions)\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_remaining_functions()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\saved_model\\load.py:327\u001b[0m, in \u001b[0;36mLoader._load_edges\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Adds edges from objects to other objects and functions.\"\"\"\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39mfor\u001b[39;00m node_id, object_proto \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_all_nodes():\n\u001b[1;32m--> 327\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_object_graph_edges(object_proto, node_id)\n\u001b[0;32m    329\u001b[0m \u001b[39m# If root object isn't loaded, then create edges from the root for\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39m# checkpoint compatibility.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filtered_nodes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filtered_nodes:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\saved_model\\load.py:350\u001b[0m, in \u001b[0;36mLoader._add_object_graph_edges\u001b[1;34m(self, proto, node_id)\u001b[0m\n\u001b[0;32m    347\u001b[0m setter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_node_setters[node_id]\n\u001b[0;32m    349\u001b[0m \u001b[39mfor\u001b[39;00m reference \u001b[39min\u001b[39;00m proto\u001b[39m.\u001b[39mchildren:\n\u001b[1;32m--> 350\u001b[0m   setter(obj, reference\u001b[39m.\u001b[39;49mlocal_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nodes[reference\u001b[39m.\u001b[39;49mnode_id])\n\u001b[0;32m    351\u001b[0m   \u001b[39m# Note: if an object has an attribute `__call__` add a class method\u001b[39;00m\n\u001b[0;32m    352\u001b[0m   \u001b[39m# that allows `obj()` syntax to work. This is done per-instance to\u001b[39;00m\n\u001b[0;32m    353\u001b[0m   \u001b[39m# allow `callable` to be used to find out if an object is callable.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m   \u001b[39mif\u001b[39;00m reference\u001b[39m.\u001b[39mlocal_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__call__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(obj):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py:1046\u001b[0m, in \u001b[0;36m_revive_setter\u001b[1;34m(layer, name, value)\u001b[0m\n\u001b[0;32m   1044\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, trackable\u001b[39m.\u001b[39mTrackable):\n\u001b[0;32m   1045\u001b[0m     layer\u001b[39m.\u001b[39m_track_trackable(value, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m-> 1046\u001b[0m   layer\u001b[39m.\u001b[39;49m_serialized_attributes[name] \u001b[39m=\u001b[39m value\n\u001b[0;32m   1047\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39melif\u001b[39;00m (\u001b[39misinstance\u001b[39m(layer, functional_lib\u001b[39m.\u001b[39mFunctional) \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m       re\u001b[39m.\u001b[39mmatch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m^layer(_with_weights)?-[\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+]\u001b[39m\u001b[39m'\u001b[39m, name) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1050\u001b[0m   \u001b[39m# Edges named \"layer-n\" or \"layer_with_weights-n\", which are tracked in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1058\u001b[0m   \u001b[39m# original layer-n, but we already warn the users about this\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m   \u001b[39m# (ctrl-f \"shared between different layers/models\").\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RootMeanSquaredError' object has no attribute '_serialized_attributes'"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras import layers, models\n",
    "model = models.load_model('C:/Users/13764/Documents/academics/zhejiang/GP-WP/GP-WP data/models\\Logp')\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
